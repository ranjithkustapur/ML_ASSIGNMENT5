# -*- coding: utf-8 -*-
"""ML_Assignment5

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fEz_pHAL-OePvyG1CZr-rjNdLJGBGZiS
"""

# import the libraries 
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from sklearn import preprocessing, metrics
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
sns.set(style="white", color_codes=True)
import warnings
warnings.filterwarnings("ignore")

#Question-1: Principal Component Analysis
df = pd.read_csv(r"CC GENERAL.csv")

df.head()

df.isnull().any()

df.fillna(df.mean(), inplace=True)
df.isnull().any()

x = df.iloc[:,1:-1]
y = df.iloc[:,-1]
print(x.shape,y.shape)

# a. Apply PCA on CC dataset.
#Datasets can be analyzed with PCA so that redundant features can be removed without losing too much information.
pca = PCA(3)   #Instantiate PCA
x_pca = pca.fit_transform(x)
principalDf = pd.DataFrame(data = x_pca, columns = ['principal component 1', 'principal component 2', 'principal component 3'])
finalDf = pd.concat([principalDf, df.iloc[:,-1]], axis = 1)
finalDf.head()

# b. Apply k-means algorithm on the PCA result and report your observation if the silhouette score has improved or not?

X = finalDf.iloc[:,0:-1]
y = finalDf.iloc[:,-1]

nclusters = 3 # this is the k in kmeans
km = KMeans(n_clusters=nclusters)
km.fit(X)

# predict the cluster for each data point
y_cluster_kmeans = km.predict(X)

# Summary of the predictions made by the classifier
print(classification_report(y, y_cluster_kmeans, zero_division=1))
print(confusion_matrix(y, y_cluster_kmeans))

#finding the accuracy
train_accuracy = accuracy_score(y, y_cluster_kmeans)
print("\nAccuracy for our Training dataset with PCA:", train_accuracy)

#Calculating sihouette Score
score = metrics.silhouette_score(X, y_cluster_kmeans)
print("Sihouette Score: ",score)   #ranges from -1 to +1, high value shows that it is matched more

# c. Perform Scaling+PCA+K-Means and report performance.

x = df.iloc[:,1:-1]
y = df.iloc[:,-1]
print(x.shape,y.shape)

## Scale the dataset; This is very important before you apply PCA
scaler = StandardScaler()
scaler.fit(x)
X_scaled_array = scaler.transform(x)

# Instantiate PCA
pca = PCA(3)

# Determine transformed features
x_pca = pca.fit_transform(X_scaled_array)
principalDf = pd.DataFrame(data = x_pca, columns = ['principal component 1', 'principal component 2','principal component 3'])
finalDf = pd.concat([principalDf, df.iloc[:,-1]], axis = 1)
finalDf.head()

x = finalDf.iloc[:,0:-1]
y = finalDf["TENURE"]
print(X.shape,y.shape)

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.34,random_state=0)
nclusters = 3 
# this is the k in kmeans
km = KMeans(n_clusters=nclusters)
km.fit(X_train,y_train)


# predict the cluster for each training data point
y_clus_train = km.predict(X_train)

# Summary of the predictions made by the classifier
print(classification_report(y_train, y_clus_train, zero_division=1))
print(confusion_matrix(y_train, y_clus_train))

train_accuracy = accuracy_score(y_train, y_clus_train)
print("Accuracy for our Training dataset with PCA:", train_accuracy)

#Calculating sihouette Score
score = metrics.silhouette_score(X_train, y_clus_train)
print("Sihouette Score: ",score)   #ranges from -1 to +1, high value shows that it is matched more

# predict the cluster for each testing data point
y_clus_test = km.predict(X_test)

# Summary of the predictions made by the classifier
print(classification_report(y_test, y_clus_test, zero_division=1))
print(confusion_matrix(y_test, y_clus_test))

train_accuracy = accuracy_score(y_test, y_clus_test)
print("\nAccuracy for our Training dataset with PCA:", train_accuracy)

#Calculating sihouette Score
score = metrics.silhouette_score(X_test, y_clus_test)
print("Sihouette Score: ",score)   #ranges from -1 to +1, high value shows that it is matched more

#Question-2: Use pd_speech_features.csv

df_pd = pd.read_csv(r"pd_speech_features.csv")

df_pd.head()

df_pd.isnull().any()

X = df_pd.drop('class',axis=1).values
Y = df_pd['class'].values

# a. Perform Scaling

#Scaling Data
scaler = StandardScaler()
X_Scale = scaler.fit_transform(X)

# b. Apply PCA (k=3)

# Apply PCA with k =3
pca3 = PCA(n_components=3)
principalComponents = pca3.fit_transform(X_Scale)

principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2','Principal Component 3'])

finalDf = pd.concat([principalDf, df_pd[['class']]], axis = 1)
finalDf.head()

X = finalDf.drop('class',axis=1).values
Y = finalDf['class'].values
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3,random_state=0)

# c. Use SVM to report performance

from sklearn.svm import SVC

svmClassifier = SVC()
svmClassifier.fit(X_train, Y_train)

y_pred = svmClassifier.predict(X_test)

# Summary of the predictions made by the classifier
print(classification_report(Y_test, y_pred, zero_division=1))
print(confusion_matrix(Y_test, y_pred))
# Accuracy score
glass_acc_svc = accuracy_score(y_pred,Y_test)
print('accuracy is',glass_acc_svc)

#Calculate sihouette Score
score = metrics.silhouette_score(X_test, y_pred)
print("Sihouette Score: ",score)

#Question-3: Apply Linear Discriminant Analysis (LDA) on Iris.csv dataset to reduce dimensionality of data to k=2. 
#A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes’ rule.
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
df_iris = pd.read_csv(r"Iris.csv")

df_iris.isnull().any()

x = df_iris.iloc[:,1:-1]
y = df_iris.iloc[:,-1]
print(x.shape,y.shape)

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0)

sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
le = LabelEncoder()
y = le.fit_transform(y)

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
lda = LDA(n_components=2)
X_train = lda.fit_transform(X_train, y_train)
X_test = lda.transform(X_test)
print(X_train.shape,X_test.shape)

#Question-4: Briefly identify the difference between PCA and LDA
PCA(Principal Component Analysis) - Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional 
      space. The input data is centered but not scaled for each feature before applying the SVD. 
      It uses the LAPACK implementation of the full SVD or a randomized truncated SVD.
LDA(Linear Discriminant Analysis) - A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and 
      using Bayes’ rule. The model fits a Gaussian density to each class, if all classes share the same covariance matrix.
Both LDA and PCA are linear transformation techniques: LDA is a supervised whereas PCA is unsupervised – PCA ignores class
labels. In contrast to PCA, LDA attempts to find a feature subspace that maximizes class separability. They both try to 
reduce dimension of data by projecting onto new space which has less dimension than the formers. They both rank new 
axes in order of importance. This importance is measured by eigenvalues. From both, we can know that which features 
are important and contributing more to creating new axes.